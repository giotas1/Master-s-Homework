{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELLENIC OPEN UNIVERSITY - SCHOOL OF SCIENCE AND TECHNOLOGY\n",
    "### DATA SCIENCE AND MACHINE LEARNING : DAMA61 ACAD. YEAR 2023-24\n",
    "\n",
    "#### <center> WRITTEN ASSIGNMENT 1 - SOLUTIONS </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the width of the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Work with all the data of the data frame of Life satisfaction and the GDP per capita (full_country_stats).\n",
    "\n",
    "1) Apply polynomial regression with a polynomial degree 8 and plot your results.</br>\n",
    "2) Calculate the life satisfaction index (LSI) for a country with GDP equal to 97000.</br>\n",
    "3) Use the three nearest neighbours to the country with GDP equal to 97000 to estimate its LSI.</br>\n",
    "4) Compare the predictions of the two approaches. Comment on your results. </br>\n",
    "5) Now, make predictions by using polynomials with degree from 1 to 10. What do you observe? Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set the plotting parameters\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "download_root = \"https://github.com/ageron/data/raw/main/\"\n",
    "\n",
    "datapath = os.path.join(\"datasets\", \"lifesat\")\n",
    "os.makedirs(datapath, exist_ok=True)\n",
    "\n",
    "for filename in (\"oecd_bli.csv\", \"gdp_per_capita.csv\"):\n",
    "    print(f\"Downloading file: {filename}\")\n",
    "    url = download_root + \"lifesat/\" + filename\n",
    "    urllib.request.urlretrieve(url, os.path.join(datapath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "oecd_bli = pd.read_csv(os.path.join(datapath, \"oecd_bli.csv\"))\n",
    "gdp_per_capita = pd.read_csv(os.path.join(datapath, \"gdp_per_capita.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 lines of the data in each file\n",
    "oecd_bli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_per_capita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "gdp_year = 2020\n",
    "gdppc_col = \"GDP per capita (USD)\"\n",
    "lifesat_col = \"Life satisfaction\"\n",
    "\n",
    "gdp_per_capita = gdp_per_capita[gdp_per_capita[\"Year\"] == gdp_year]\n",
    "gdp_per_capita = gdp_per_capita.drop([\"Code\", \"Year\"], axis=1)\n",
    "gdp_per_capita.columns = [\"Country\", gdppc_col]\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "\n",
    "gdp_per_capita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"] == \"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "\n",
    "oecd_bli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                              left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=gdppc_col, inplace=True)\n",
    "full_country_stats = full_country_stats[[gdppc_col, lifesat_col]]\n",
    "\n",
    "full_country_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the traing set\n",
    "Xfull = np.c_[full_country_stats[\"GDP per capita (USD)\"]]\n",
    "yfull = np.c_[full_country_stats[\"Life satisfaction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "poly = PolynomialFeatures(degree=8, include_bias=False) \n",
    "scaler = StandardScaler()\n",
    "model2 = LinearRegression()\n",
    "\n",
    "pl_reg = Pipeline([('poly', poly), ('scal', scaler), ('lin', model2)])\n",
    "pl_reg.fit(Xfull, yfull)\n",
    "\n",
    "full_country_stats.plot(kind=\"scatter\", x=\"GDP per capita (USD)\", \n",
    "                        y=\"Life satisfaction\", figsize=(8,3))\n",
    "\n",
    "plt.axis([0, 120000, -2, 10])\n",
    "\n",
    "X = np.linspace(0, 120000, 1000)\n",
    "plt.plot(X, pl_reg.predict(X.reshape(-1,1)), \"k--\")\n",
    "\n",
    "# Answer 2\n",
    "X0 = 97000\n",
    "ls_X0 = pl_reg.predict([[X0]])\n",
    "plt.plot(X0, ls_X0, \"r*\", markersize = 15)\n",
    "plt.annotate(f\"({X0}, {ls_X0[0][0]:.1f})\", xy=(X0, ls_X0-0.25), xytext=(X0-28000, ls_X0-1.5),\n",
    "             arrowprops=dict(facecolor=\"black\", width=0.25, shrink=0.1, headwidth=6))\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 3\n",
    "X0 = 97000\n",
    "\n",
    "# find the three nearest neighbours\n",
    "X = abs(Xfull - X0)\n",
    "indices = X.flatten().argsort()\n",
    "\n",
    "# print the GDP and the life satisfaction index (LSI) of the three nearest neighbours\n",
    "print(\"GDP:\", Xfull[indices[:3]].flatten())\n",
    "print(\"LSI:\", yfull[indices[:3]].flatten())\n",
    "\n",
    "# print the \n",
    "print(f\"Predicted LSI: {np.mean(yfull[indices[:3]]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "By using a polynomial of degree 8 we estimate the LSI of a country with GDP equal to 97000 to be about 2. While by using the LSI of the three nearest neighbours (in terms of GDP) we predict a value of 7.1, which is a reasonable value. This result reveals that the polynomial of degree 8 fits the training data very closely (overfits) but fails to generalize well to areas where there are unknown data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "full_country_stats.plot(kind=\"scatter\", x=\"GDP per capita (USD)\",\n",
    "                        y=\"Life satisfaction\", figsize=(8,6))\n",
    "\n",
    "plt.axis([0, 120000, -2, 10])\n",
    "X = np.linspace(0, 120000, 1000)\n",
    "\n",
    "for poly_degree in range(1,11):\n",
    "    poly = PolynomialFeatures(degree=poly_degree, include_bias=False) \n",
    "    scaler = StandardScaler()\n",
    "    model2 = LinearRegression()\n",
    "\n",
    "    pl_reg = Pipeline([('poly', poly), ('scal', scaler), ('lin', model2)])\n",
    "    pl_reg.fit(Xfull, yfull)\n",
    "\n",
    "    plt.plot(X, pl_reg.predict(X.reshape(-1,1)), color = f\"C{poly_degree}\", \n",
    "             ls=\"--\", label=f\"degree:{poly_degree}\")\n",
    "\n",
    "    X0 = 97000\n",
    "    plt.plot(X0, pl_reg.predict([[X0]]), \"r*\", markersize = 15)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "By trying several values for the degree of a polynomial we observe that as the degree increases the model overfits the training data and again losses its ability to generalize well to unknown values of the GDP. This means that the model becomes less reliable in predicting life satisfaction for countries with GDP values that were not included in the training data. Therefore, it is important to find an appropriate balance in choosing the degree of the polynomial to prevent overfitting and ensure better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Work with the data used to predict median house values in Californian districts (file: housing.tgz).\n",
    "\n",
    "1) Use a Random Forest Regressor as your estimator and perform Grid Search cross-validation with parameters: </br>\n",
    "n_estimators: [5, 15, 25], and max_features: [2, 4, 8].</br>\n",
    "   What are the parameters of your best model? Are you satisfied with your results? </br>\n",
    "\n",
    "2) What are the three most important features of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed packages\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the data\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create income_cat as a new feature based on the median_income\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stratified split to create the training and test sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target value of this problem and remove it from the input features \n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n",
    "housing = strat_train_set.drop(['median_house_value', 'income_cat'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to dummify the categorical variable  \n",
    "housing_cat = housing[[\"ocean_proximity\"]].copy()\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the numeric and the categorical features\n",
    "num_features = list(housing_num)\n",
    "cat_features = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), # or \"most_frequent\"\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_features),\n",
    "        (\"cat\", OneHotEncoder(), cat_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    # create a Random Forest Regressor model as part of the pipeline\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42))])\n",
    "\n",
    "# set the parameters grid\n",
    "param_grid = [\n",
    "    {'random_forest__n_estimators': [5, 15, 25],\n",
    "     'random_forest__max_features': [2, 4, 8]}]\n",
    "\n",
    "# use GridSearchCV to apply 3-fold cv\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
    "                           scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "# fit the grid to find the best parameters\n",
    "grid_search.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best parameters\n",
    "print (best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "We observe that both the maximum number of features and the number of estimators of the best model take values equal to the upper limit of the values considered during the grid search. This result suggests that it may be beneficial to increase the upper limits of the hyperparameters and rerun the calculation in order to potentially achieve better results. Therefore, we can conclude that the current results are unsatisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_model['random_forest'].feature_importances_\n",
    "\n",
    "sorted(zip(feature_importances,\n",
    "           best_model['preprocessing'].get_feature_names_out()),\n",
    "           reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "Based on our analysis and the best parameters, we conclude that the three most important features are median income, INLAND, and longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Work with the MNIST dataset and:\n",
    "\n",
    "1) Load the data and split them into training (6/7) and test (1/7) sets.</br>\n",
    "2) Train a binary classifier of your choice to distinguish between two classes, 4 and not-4.</br>\n",
    "3) Use 3-fold cross validation and evaluate your model by calculating the metrics: accuracy, recall, and precision. Compare the accuracy of your model to the accuracy of a model that always guesses that an image is not a 4.</br>\n",
    "4) Calculate the confusion matrix for the train set. How many of the train samples were wrongly classified as 4s and how many wrongly classified as non-4s?</br>\n",
    "5) Plot the ROC curve and calculate the area under the curve (AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target values\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to training (6/7) and test sets (1/7)\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first digit and its label value of the training set\n",
    "plt.imshow(X_train[0].reshape((28,28)))\n",
    "plt.show()\n",
    "print(f\"This is a {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 4 and not-4 target values\n",
    "y_train_4 = (y_train == '4')\n",
    "y_test_4 = (y_test == '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first digit and its label value of the training set\n",
    "plt.imshow(X_train[0].reshape((28,28)))\n",
    "plt.show()\n",
    "print(f\"This is {'a four' if y_train_4[0] else 'not a four'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an SGD classifier \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy, recall and precision of the training set\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(sgd_clf, X_train, y_train_4, cv=3,\n",
    "                        scoring=['accuracy', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of each fold: {scores['test_accuracy']}, mean accuracy: {100*scores['test_accuracy'].mean():.1f}%\")\n",
    "print(f\"Recall of each fold: {scores['test_recall']}, mean recall: {100*scores['test_recall'].mean():.1f}%\")\n",
    "print(f\"Precision of each fold: {scores['test_precision']}, mean precision: {100*scores['test_precision'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an easy way to calculate the accuracy of a model that always guesses an image is not a 4, you can sum up all the correct guesses,\n",
    "# i.e., the non-4 images in the dataset, and divide that by the total number of images. \n",
    "non_4_accuracy = (y_train_4 == 0).sum()/len(y_train_4)\n",
    "print (f\"Accuracy of a model that always guesses not a 4: {100*non_4_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "The accuracy of the model that always guesses an image is not a 4 is 90.3%.</br>\n",
    "This example demonstrates that when evaluating models, it's crucial to consider the balance of the dataset and be cautious when using accuracy as the sole metric.</br>\n",
    "Accuracy can be misleading if the classes are imbalanced, meaning one class is significantly more prevalent than the other. In such cases, a model that always predicts the majority class can achieve high accuracy even without truly learning the underlying patterns.</br>\n",
    "To overcome this limitation, it's important to consider additional evaluation metrics, such as precision, recall, F1 score, or area under the receiver operating characteristic curve (AUC-ROC). These metrics provide a more comprehensive assessment of model performance, especially when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_4, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_4, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "The confusion matrix shows that the model made 52,957 correct negative (TN) predictions, 1,201 false positive (FP) predictions, 554 false negative (FN) predictions, and 5,288 correct positive (TP) predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_4, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_4, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.figure(figsize=(8, 6))                  \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random\")\n",
    "    plt.axis([0, 1, 0, 1])              \n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plot_roc_curve(fpr, tpr, \"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the area under the curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_4, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "Both the ROC and the area under the curve demonstrate that our model performs well on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "22b0ec00cd9e253c751e6d2619fc0bb2d18ed12980de3246690d5be49479dd65"
   }
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
