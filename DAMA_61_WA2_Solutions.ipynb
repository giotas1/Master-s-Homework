{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELLENIC OPEN UNIVERSITY - SCHOOL OF SCIENCE AND TECHNOLOGY\n",
    "### DATA SCIENCE AND MACHINE LEARNING : DAMA61 ACAD. YEAR 2023-24\n",
    "\n",
    "#### <center> WRITTEN ASSIGNMENT 2 - SOLUTIONS </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the width of the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Use the following code to create a set of non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "m = 1000\n",
    "X = 10 * np.random.rand(m, 1) - 5\n",
    "y = 2*X + X**2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Transform the data using a polynomial of degree four.</br>\n",
    "2) Train a Lasso regularized linear regression models with alpha 0.01 on the polynomial features.</br>\n",
    "3) What are the coefficients each lasso regularized model predicts? Comment on your results.</br>\n",
    "4) Train a Ridge regularized linear regression models with alpha 0.01 on the polynomial features.</br>\n",
    "5) What are the coefficients the ridge regularized model predicts? Comment on your results.</br>\n",
    "<i>Hint: Check the API Documentation of <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\">Lasso</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\">Ridge</a> models.</i> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data\n",
    "import numpy as np\n",
    "np.random.seed(40)\n",
    "m = 1000\n",
    "X = 10 * np.random.rand(m, 1) - 5\n",
    "y = 2 * X + X**2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "plt.plot(X, y, \"o\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data to polynomial features of degree 4\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=4, include_bias=False)\n",
    "x_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Lasso regularizer with alpha 0.01\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(0.01)\n",
    "lasso.fit(x_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and display the data and the results\n",
    "y_pred = lasso.predict(x_poly)\n",
    "\n",
    "plt.plot(X, y, \"b*\", label=\"Ground Truth\")\n",
    "plt.plot(X, y_pred, \"ro\", label=\"Predicted\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "Our analysis assumed a polynomial of degree 4 to fit the data. We observe that the model retrieves the factors of the first and second-order features while eliminating the contribution of the third and fourth-order features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Ridge regularizer with alpha 0.01\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(0.01)\n",
    "ridge.fit(x_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and display the data and the results\n",
    "y_pred = ridge.predict(x_poly)\n",
    "\n",
    "plt.plot(X, y, \"b*\", label=\"Ground Truth\")\n",
    "plt.plot(X, y_pred, \"ro\", label=\"Predicted\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "Like the Lasso model, we observe that the Ridge one retrieves the factors of the first and second-order features while eliminating the contribution of the third and fourth-order features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Work with the Iris dataset to create an SVM classifier to distinguish Iris-Setosa and Iris-Virginica samples:\n",
    "\n",
    "1) Load the data and keep only the sepal length and sepal width features, and the Iris-Setosa and Iris-Virginica target values.</br>\n",
    "2) Visualize the data on a plot displaing the sepal length on the x-axis and the sepal width on the y-axis.</br>\n",
    "3) Train two linear SVM classifiers with the regularization hyperparameter C equal to 10 and 100, respectively.</br>\n",
    "4) Which of the values of C concludes to a more reliable model? Comment on your results.\n",
    "5) Use your choice of the value of C from the previous question and train a Logistic Regression model to classify the two classes again.</br>\n",
    "6) Use a contour plot to visualize the probability of a sample being Iris-Setosa or Iris-Virginica for all the values of sepal lengths and sepal widths in the range of their minimum and maximum value, respectively.</br>\n",
    "7) What is the probability of a sample being Iris setosa given that its sepal length is 5.5 cm and its sepal width is 3.25 cm?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# keep the sepal-length and sepal-width features\n",
    "X = iris.data[[\"sepal length (cm)\", \"sepal width (cm)\"]].values\n",
    "y = iris.target\n",
    "\n",
    "# keep the setosa and virginica samples\n",
    "setosa_or_virginica = (y == 0) | (y == 2)\n",
    "X = X[setosa_or_virginica]\n",
    "y = y[setosa_or_virginica]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the data\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Setosa\")\n",
    "plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"ro\", label=\"Iris-Versicolor\")\n",
    "plt.xlabel(\"Sepal length (cm)\")\n",
    "plt.ylabel(\"Sepal width (cm)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that draws the decision boundary of a given SVM classifier\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0] / w[1] * x0 - b / w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "    svs = svm_clf.support_vectors_\n",
    "\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2, zorder=-2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2, zorder=-2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2, zorder=-2)\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#AAA', zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train an SVM classifier with linear kernel and C = 10\n",
    "svm_clf10 = SVC(kernel=\"linear\", C=10)\n",
    "svm_clf10.fit(X, y)\n",
    "\n",
    "# train an SVM classifier with linear kernel and C = 100\n",
    "svm_clf100 = SVC(kernel=\"linear\", C=100)\n",
    "svm_clf100.fit(X, y)\n",
    "\n",
    "# visualize our results\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"C = 10\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Setosa\")\n",
    "plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"ro\", label=\"Iris-Versicolor\")\n",
    "\n",
    "# plot the decision boundary for C=10\n",
    "plot_svc_decision_boundary(svm_clf10, min(X[:,0]), max(X[:,0]))\n",
    "plt.xlabel(\"Sepal length (cm)\")\n",
    "plt.ylabel(\"Sepal width (cm)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"C = 100\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Setosa\")\n",
    "plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"ro\", label=\"Iris-Versicolor\")\n",
    "\n",
    "# plot the decision boundary for C=100\n",
    "plot_svc_decision_boundary(svm_clf100, min(X[:,0]), max(X[:,0]))\n",
    "\n",
    "plt.xlabel(\"Sepal length (cm)\")\n",
    "plt.ylabel(\"Sepal width (cm)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "\n",
    "We observe that the lower value of C, i.e., C=10, concludes a more reliable and generalizable model, keeping the street as large as possible while eliminating the margin violations. On the other hand, C equal to 100, is very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "softmax_reg = LogisticRegression(C=10, random_state=42)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lengths, widths = np.meshgrid(np.linspace(min(X[:,0]), max(X[:,0]), 500).reshape(-1, 1),\n",
    "                              np.linspace(min(X[:,1]), max(X[:,1]), 200).reshape(-1, 1))\n",
    "\n",
    "X_new = np.c_[lengths.ravel(), widths.ravel()]\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "probs = y_proba[:, 1].reshape(lengths.shape)\n",
    "classes = y_predict.reshape(lengths.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris-Setosa\")\n",
    "plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"ro\", label=\"Iris-Versicolor\")\n",
    "\n",
    "plt.contourf(lengths, widths, probs, alpha=0.5, cmap=\"bwr\")\n",
    "contour = plt.contour(lengths, widths, classes, cmap=\"bwr\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The probability of a sample being Iris setosa given that its sepal length is 5.5 cm and its sepal width is 3.25 cm is {100*softmax_reg.predict_proba([[5.5, 3.25]])[0][0]:.2f}%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "22b0ec00cd9e253c751e6d2619fc0bb2d18ed12980de3246690d5be49479dd65"
   }
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
